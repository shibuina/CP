{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mai' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mai ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "class Q_Learning:\n",
    "    ###########################################################################\n",
    "    #   START - __init__ function\n",
    "    ###########################################################################\n",
    "    # INPUTS: \n",
    "    # env - Cart Pole environment\n",
    "    # alpha - step size \n",
    "    # gamma - discount rate\n",
    "    # epsilon - parameter for epsilon-greedy approach\n",
    "    # numberEpisodes - total number of simulation episodes\n",
    "     \n",
    "    # numberOfBins - this is a 4 dimensional list that defines the number of grid points \n",
    "    # for state discretization\n",
    "    # that is, this list contains number of bins for every state entry, \n",
    "    # we have 4 entries, that is,\n",
    "    # discretization for cart position, cart velocity, pole angle, and pole angular velocity\n",
    "     \n",
    "    # lowerBounds - lower bounds (limits) for discretization, list with 4 entries:\n",
    "    # lower bounds on cart position, cart velocity, pole angle, and pole angular velocity\n",
    " \n",
    "    # upperBounds - upper bounds (limits) for discretization, list with 4 entries:\n",
    "    # upper bounds on cart position, cart velocity, pole angle, and pole angular velocity\n",
    "     \n",
    "    def __init__(self,env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds):\n",
    "        import numpy as np\n",
    "         \n",
    "        self.env=env\n",
    "        self.alpha=alpha\n",
    "        self.gamma=gamma \n",
    "        self.epsilon=epsilon \n",
    "        self.actionNumber=env.action_space.n \n",
    "        self.numberEpisodes=numberEpisodes\n",
    "        self.numberOfBins=numberOfBins\n",
    "        self.lowerBounds=lowerBounds\n",
    "        self.upperBounds=upperBounds\n",
    "         \n",
    "        # this list stores sum of rewards in every learning episode\n",
    "        self.sumRewardsEpisode=[]\n",
    "         \n",
    "        # this matrix is the action value function matrix \n",
    "        self.Qmatrix=np.random.uniform(low=0, high=1, size=(numberOfBins[0],numberOfBins[1],numberOfBins[2],numberOfBins[3],self.actionNumber))\n",
    "         \n",
    "     \n",
    "    ###########################################################################\n",
    "    #   END - __init__ function\n",
    "    ###########################################################################\n",
    "     \n",
    "    ###########################################################################\n",
    "    # START: function \"returnIndexState\"\n",
    "    # for the given 4-dimensional state, and discretization grid defined by \n",
    "    # numberOfBins, lowerBounds, and upperBounds, this function will return \n",
    "    # the index tuple (4-dimensional) that is used to index entries of the \n",
    "    # of the QvalueMatrix \n",
    " \n",
    " \n",
    "    # INPUTS:\n",
    "    # state - state list/array, 4 entries: \n",
    "    # cart position, cart velocity, pole angle, and pole angular velocity\n",
    " \n",
    "    # OUTPUT: 4-dimensional tuple defining the indices of the QvalueMatrix \n",
    "    # that correspond to \"state\" input\n",
    " \n",
    "    ###############################################################################\n",
    "    def returnIndexState(self,state):\n",
    "        position =      state[0]\n",
    "        velocity =      state[1]\n",
    "        angle    =      state[2]\n",
    "        angularVelocity=state[3]\n",
    "         \n",
    "        cartPositionBin=np.linspace(self.lowerBounds[0],self.upperBounds[0],self.numberOfBins[0])\n",
    "        cartVelocityBin=np.linspace(self.lowerBounds[1],self.upperBounds[1],self.numberOfBins[1])\n",
    "        poleAngleBin=np.linspace(self.lowerBounds[2],self.upperBounds[2],self.numberOfBins[2])\n",
    "        poleAngleVelocityBin=np.linspace(self.lowerBounds[3],self.upperBounds[3],self.numberOfBins[3])\n",
    "         \n",
    "        indexPosition=np.maximum(np.digitize(state[0],cartPositionBin)-1,0)\n",
    "        indexVelocity=np.maximum(np.digitize(state[1],cartVelocityBin)-1,0)\n",
    "        indexAngle=np.maximum(np.digitize(state[2],poleAngleBin)-1,0)\n",
    "        indexAngularVelocity=np.maximum(np.digitize(state[3],poleAngleVelocityBin)-1,0)\n",
    "         \n",
    "        return tuple([indexPosition,indexVelocity,indexAngle,indexAngularVelocity])   \n",
    "    ###########################################################################\n",
    "    #   END - function \"returnIndexState\"\n",
    "    ###########################################################################    \n",
    "        \n",
    "    ###########################################################################\n",
    "    #    START - function for selecting an action: epsilon-greedy approach\n",
    "    ###########################################################################\n",
    "    # this function selects an action on the basis of the current state \n",
    "    # INPUTS: \n",
    "    # state - state for which to compute the action\n",
    "    # index - index of the current episode\n",
    "    def selectAction(self,state,index):\n",
    "         \n",
    "        # first 500 episodes we select completely random actions to have enough exploration\n",
    "        if index<500:\n",
    "            return np.random.choice(self.actionNumber)   \n",
    "             \n",
    "        # Returns a random real number in the half-open interval [0.0, 1.0)\n",
    "        # this number is used for the epsilon greedy approach\n",
    "        randomNumber=np.random.random()\n",
    "         \n",
    "        # after 7000 episodes, we slowly start to decrease the epsilon parameter\n",
    "        if index>7000:\n",
    "            self.epsilon=0.999*self.epsilon\n",
    "         \n",
    "        # if this condition is satisfied, we are exploring, that is, we select random actions\n",
    "        if randomNumber < self.epsilon:\n",
    "            # returns a random action selected from: 0,1,...,actionNumber-1\n",
    "            return np.random.choice(self.actionNumber)            \n",
    "         \n",
    "        # otherwise, we are selecting greedy actions\n",
    "        else:\n",
    "            # we return the index where Qmatrix[state,:] has the max value\n",
    "            # that is, since the index denotes an action, we select greedy actions\n",
    "            return np.random.choice(np.where(self.Qmatrix[self.returnIndexState(state)]==np.max(self.Qmatrix[self.returnIndexState(state)]))[0])\n",
    "            # here we need to return the minimum index since it can happen\n",
    "            # that there are several identical maximal entries, for example \n",
    "            # import numpy as np\n",
    "            # a=[0,1,1,0]\n",
    "            # np.where(a==np.max(a))\n",
    "            # this will return [1,2], but we only need a single index\n",
    "            # that is why we need to have np.random.choice(np.where(a==np.max(a))[0])\n",
    "            # note that zero has to be added here since np.where() returns a tuple\n",
    "    ###########################################################################\n",
    "    #    END - function selecting an action: epsilon-greedy approach\n",
    "    ###########################################################################\n",
    "     \n",
    "     \n",
    "    ###########################################################################\n",
    "    #    START - function for simulating learning episodes\n",
    "    ###########################################################################\n",
    "      \n",
    "    def simulateEpisodes(self):\n",
    "        import numpy as np\n",
    "        # here we loop through the episodes\n",
    "        for indexEpisode in range(self.numberEpisodes):\n",
    "             \n",
    "            # list that stores rewards per episode - this is necessary for keeping track of convergence \n",
    "            rewardsEpisode=[]\n",
    "             \n",
    "            # reset the environment at the beginning of every episode\n",
    "            (stateS,_)=self.env.reset()\n",
    "            stateS=list(stateS)\n",
    "           \n",
    "            print(\"Simulating episode {}\".format(indexEpisode))\n",
    "             \n",
    "             \n",
    "            # here we step from one state to another\n",
    "            # this will loop until a terminal state is reached\n",
    "            terminalState=False\n",
    "            while not terminalState:\n",
    "                # return a discretized index of the state\n",
    "                 \n",
    "                stateSIndex=self.returnIndexState(stateS)\n",
    "                 \n",
    "                # select an action on the basis of the current state, denoted by stateS\n",
    "                actionA = self.selectAction(stateS,indexEpisode)\n",
    "                 \n",
    "                 \n",
    "                # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "                # prime means that it is the next state\n",
    "                (stateSprime, reward, terminalState,_,_) = self.env.step(actionA)          \n",
    "                 \n",
    "                rewardsEpisode.append(reward)\n",
    "                 \n",
    "                stateSprime=list(stateSprime)\n",
    "                 \n",
    "                stateSprimeIndex=self.returnIndexState(stateSprime)\n",
    "                 \n",
    "                # return the max value, we do not need actionAprime...\n",
    "                QmaxPrime=np.max(self.Qmatrix[stateSprimeIndex])                                               \n",
    "                                              \n",
    "                if not terminalState:\n",
    "                    # stateS+(actionA,) - we use this notation to append the tuples\n",
    "                    # for example, for stateS=(0,0,0,1) and actionA=(1,0)\n",
    "                    # we have stateS+(actionA,)=(0,0,0,1,0)\n",
    "                    error=reward+self.gamma*QmaxPrime-self.Qmatrix[stateSIndex+(actionA,)]\n",
    "                    self.Qmatrix[stateSIndex+(actionA,)]=self.Qmatrix[stateSIndex+(actionA,)]+self.alpha*error\n",
    "                else:\n",
    "                    # in the terminal state, we have Qmatrix[stateSprime,actionAprime]=0 \n",
    "                    error=reward-self.Qmatrix[stateSIndex+(actionA,)]\n",
    "                    self.Qmatrix[stateSIndex][actionA]=self.Qmatrix[stateSIndex+(actionA,)]+self.alpha*error\n",
    "                 \n",
    "                # set the current state to the next state                    \n",
    "                stateS=stateSprime\n",
    "         \n",
    "            print(\"Sum of rewards {}\".format(np.sum(rewardsEpisode)))        \n",
    "            self.sumRewardsEpisode.append(np.sum(rewardsEpisode))\n",
    "  \n",
    "         \n",
    "    ###########################################################################\n",
    "    #    END - function for simulating learning episodes\n",
    "    ###########################################################################\n",
    "     \n",
    "     \n",
    "    ###########################################################################\n",
    "    #    START - function for simulating the final learned optimal policy\n",
    "    ###########################################################################\n",
    "    # OUTPUT: \n",
    "    # env1 - created Cart Pole environment\n",
    "    # obtainedRewards - a list of obtained rewards during time steps of a single episode\n",
    "     \n",
    "    # simulate the final learned optimal policy\n",
    "    def simulateLearnedStrategy(self):\n",
    "        import gym \n",
    "        import time\n",
    "        env1=gym.make('CartPole-v1')\n",
    "        (currentState,_)=env1.reset()\n",
    "        env1.render()\n",
    "        timeSteps=1000\n",
    "        # obtained rewards at every time step\n",
    "        obtainedRewards=[]\n",
    "         \n",
    "        for timeIndex in range(timeSteps):\n",
    "            print(timeIndex)\n",
    "            # select greedy actions\n",
    "            actionInStateS=np.random.choice(np.where(self.Qmatrix[self.returnIndexState(currentState)]==np.max(self.Qmatrix[self.returnIndexState(currentState)]))[0])\n",
    "            print(actionInStateS)\n",
    "            currentState, reward, terminated, truncated, info =env1.step(actionInStateS)\n",
    "            obtainedRewards.append(reward)   \n",
    "            time.sleep(0.05)\n",
    "            if (terminated):\n",
    "                time.sleep(1)\n",
    "                break\n",
    "        return obtainedRewards,env1\n",
    "    ###########################################################################\n",
    "    #    END - function for simulating the final learned optimal policy\n",
    "    ###########################################################################     \n",
    "                  \n",
    "    ###########################################################################\n",
    "    #    START - function for simulating random actions many times\n",
    "    #   this is used to evaluate the optimal policy and to compare it with a random policy\n",
    "    ###########################################################################\n",
    "    #  OUTPUT:\n",
    "    # sumRewardsEpisodes - every entry of this list is a sum of rewards obtained by simulating the corresponding episode\n",
    "    # env2 - created Cart Pole environment\n",
    "    def simulateRandomStrategy(self):\n",
    "        import gym \n",
    "        import time\n",
    "        import numpy as np\n",
    "        env2=gym.make('CartPole-v1')\n",
    "        (currentState,_)=env2.reset()\n",
    "        env2.render()\n",
    "        # number of simulation episodes\n",
    "        episodeNumber=100\n",
    "        # time steps in every episode\n",
    "        timeSteps=1000\n",
    "        # sum of rewards in each episode\n",
    "        sumRewardsEpisodes=[]\n",
    "         \n",
    "         \n",
    "        for episodeIndex in range(episodeNumber):\n",
    "            rewardsSingleEpisode=[]\n",
    "            initial_state=env2.reset()\n",
    "            print(episodeIndex)\n",
    "            for timeIndex in range(timeSteps):\n",
    "                random_action=env2.action_space.sample()\n",
    "                observation, reward, terminated, truncated, info =env2.step(random_action)\n",
    "                rewardsSingleEpisode.append(reward)\n",
    "                if (terminated):\n",
    "                    break     \n",
    "            sumRewardsEpisodes.append(np.sum(rewardsSingleEpisode))\n",
    "        return sumRewardsEpisodes,env2\n",
    "    ###########################################################################\n",
    "    #    END - function for simulating random actions many times\n",
    "    ###########################################################################                 \n",
    "      \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mai' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mai ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# You can either use gym (not maintained anymore) or gymnasium (maintained version of gym)    \n",
    "     \n",
    "# tested on     \n",
    "# gym==0.26.2\n",
    "# gym-notices==0.0.8\n",
    " \n",
    "#gymnasium==0.27.0\n",
    "#gymnasium-notices==0.0.1\n",
    " \n",
    "# classical gym \n",
    "import gym\n",
    "# instead of gym, import gymnasium \n",
    "# import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    " \n",
    "#env=gym.make('CartPole-v1',render_mode='human')\n",
    "env=gym.make('CartPole-v1')\n",
    "(state,_)=env.reset()\n",
    "#env.render()\n",
    "#env.close()\n",
    " \n",
    "# here define the parameters for state discretization\n",
    "upperBounds=env.observation_space.high\n",
    "lowerBounds=env.observation_space.low\n",
    "cartVelocityMin=-3\n",
    "cartVelocityMax=3\n",
    "poleAngleVelocityMin=-10\n",
    "poleAngleVelocityMax=10\n",
    "upperBounds[1]=cartVelocityMax\n",
    "upperBounds[3]=poleAngleVelocityMax\n",
    "lowerBounds[1]=cartVelocityMin\n",
    "lowerBounds[3]=poleAngleVelocityMin\n",
    " \n",
    "numberOfBinsPosition=30\n",
    "numberOfBinsVelocity=30\n",
    "numberOfBinsAngle=30\n",
    "numberOfBinsAngleVelocity=30\n",
    "numberOfBins=[numberOfBinsPosition,numberOfBinsVelocity,numberOfBinsAngle,numberOfBinsAngleVelocity]\n",
    " \n",
    "# define the parameters\n",
    "alpha=0.1\n",
    "gamma=1\n",
    "epsilon=0.2\n",
    "numberEpisodes=10000\n",
    " \n",
    "# create an object\n",
    "Q1=Q_Learning(env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds)\n",
    "# run the Q-Learning algorithm\n",
    "Q1.simulateEpisodes()\n",
    "# simulate the learned strategy\n",
    "(obtainedRewardsOptimal,env1)=Q1.simulateLearnedStrategy()\n",
    " \n",
    "plt.figure(figsize=(12, 5))\n",
    "# plot the figure and adjust the plot parameters\n",
    "plt.plot(Q1.sumRewardsEpisode,color='blue',linewidth=1)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.savefig('convergence.png')\n",
    " \n",
    " \n",
    "# close the environment\n",
    "env1.close()\n",
    "# get the sum of rewards\n",
    "np.sum(obtainedRewardsOptimal)\n",
    " \n",
    "# now simulate a random strategy\n",
    "(obtainedRewardsRandom,env2)=Q1.simulateRandomStrategy()\n",
    "plt.hist(obtainedRewardsRandom)\n",
    "plt.xlabel('Sum of rewards')\n",
    "plt.ylabel('Percentage')\n",
    "plt.savefig('histogram.png')\n",
    "plt.show()\n",
    " \n",
    "# run this several times and compare with a random learning strategy\n",
    "(obtainedRewardsOptimal,env1)=Q1.simulateLearnedStrategy()\n",
    "env1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mai' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mai ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(upperBounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mai' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mai ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mai' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n mai ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
